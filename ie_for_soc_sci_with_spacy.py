# -*- coding: utf-8 -*-
"""IE_for_soc_sci_with_spacy_solved.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13aL4xBXzkOayjf5gwhstX45C4janGm-b

# Information Extraction for Social Science Research

This tutorial will introduce you to *information extraction* for social science: techniques for turning documents into structured data by extracting specific words, phrases, or pieces of information from within documents.

----

Let's get started by installing `spaCy`, a library for doing natural language processing, and download some other data we'll need for the tutorial.
"""

!nvcc --version

!pip install --upgrade spacy

!pip install --upgrade spacy[cuda111,transformers]

!pip install jsonlines
!python -m spacy download en_core_web_lg
!python -m spacy download en_core_web_sm

!wget https://andrewhalterman.com/files/cleaned_masdar.jsonl

"""## Getting started with NER and spaCy"""

import jsonlines

from tqdm.autonotebook import tqdm
import jsonlines
import re

import spacy
from spacy import displacy
# assert spacy.__version__ == "3.1.3"

"""`spaCy` requires a pretrained model to process a document. Here, we're using the "large" model trained on English language web and news text. `spaCy` has other models including a faster `en_core_web_sm` without pretrained embeddings and `en_core_web_trf`, a transformer-based model that is more accurate but which requires more storage and more time to run. We can also load the small model in case we want to compare the speed/accuracy tradeoff of the large and small models."""

nlp = spacy.load("en_core_web_lg")
nlp_sm = spacy.load("en_core_web_sm")

"""Next, we'll load in a collection of news stories from a local pro-government newspaper in Syria called al-Masdar. The articles here primarily describe the civil war in Syria in 2016 and 2017."""

with jsonlines.open("cleaned_masdar.jsonl", "r") as f:
    articles = list(f.iter())

print(len(articles))

article = articles[313]
article

"""To process a document with `spaCy`, we'll use the `nlp` object we instatiated earlier and pass a piece of text to it. The `nlp` object returns a Document class object, which has both document and token-level attributes."""

doc = nlp(article['body'])

# take a look at how many words in a document
len(doc)

# look document-level attributes
dir(doc)

# tokens in a document can by accessed by their number:
print(doc[5])
dir(doc[5])

"""One of the attributes it assigns is named entity information for the document. Using spaCy's built-in visualizer, we can see all the detected named entities in the document.

"""

displacy.render(doc, style="ent", jupyter=True)

"""TIP: to look up what a label returned by spaCy means, you can you use the `spacy.explain()` function. So, for example,"""

spacy.explain("GPE")

"""we can use some awesome things from displacy"""

options = {"ents": ["ORG", "GPE"], "colors": {"ORG": "linear-gradient(90deg, #aa9cfc, #fc9ce7)"}}
# options = {"ents": ["ORG", "GPE"]
# colors = {"ORG": "red"}
# colors = {"ORG": "linear-gradient(90deg, #aa9cfc, #fc9ce7)"}

displacy.render(doc, style="ent", jupyter=True, options=options)

"""- Can you spot an error in the NER results?
- What if you use the small model instead of the large model? (How would you do that?)

Let's get our documents processing in the background using `spaCy`'s efficient `nlp.pipe` method and then turn to some theory and applications.
"""

just_text = [i['body'] for i in articles]
docs = list(tqdm(nlp.pipe(just_text), total=len(just_text)))

"""## NER Applications

What kinds of questions can we answer with NER and how does this fit in with our research?

The simplest questions are simple descriptive question, especially questions that could be useful at the beginning of a research project or when a research would like to understand the contents of a corpus better.

One of the simplest questions that someone to ask is which  people, organizations, and locations are mentioned most?

- **Question**: How could this be useful in research?

As an example, let's identify which organization are mentioned most in our corpus.

At this point our documents should all be processed. Do a quick check that they are:
"""

len(docs)

## refresher on the spacy objects:

# list of docs --> doc --> spans and docs --> tokens
# ent.text
# [token for token in doc]

from collections import Counter

all_orgs = []
for d in docs:
    orgs = [ent.text for ent in d.ents if ent.label_ == "ORG"]
    all_orgs.extend(orgs)

Counter(all_orgs).most_common(15)

"""**Questions**

- What does this tell us substantively about the conflict?
- How could the results be more useful? /what's missing that would make them more insightful.

### Another NER example: ceasefires and organizations

A simple extention is to study which organizations are mentioned alongside certain keywords. In practice, this would probably involve looking at organizations alongside document classifications or topics, but we can use keywords as a rough approximation here.


**Exercise**: Which organizations are mentioned most alongside mentions of "ceasefires" or "negotiations"?
"""

# write code here

# Hint: ent.sent.text will return the text of the sentence where entity `ent` is mentioned

#@title
negotiation_orgs = []
for d in docs:
    for ent in d.ents:
        if ent.label_ != "ORG":
            continue
        if re.search("negotiat|ceasefire|talks", ent.sent.text):
            negotiation_orgs.append(ent.text)

#Counter(negotiation_orgs).most_common(10)

#[('UN', 214),
# ('the United Nations Special', 159),
# ('United Nations', 107),
# ('the Syrian Opposition', 97),
# ('The Syrian Arab Army', 1),
# ('SAA', 1)]

# Caveat: what about "government"? Not an NE, so won't be there.

Counter(negotiation_orgs).most_common(10)

"""## Dependency parses

Named entity recognition is useful for identifying named entities in isolation or in the context of other terms or concepts. NER on its own tells us little about the relationships between named entities. Often, the relationship between entities is the interesting piece of information for applied researchers, and we can get at that relationship by using the grammar of the sentence.


Dependency parses are a way of representing the syntax or grammar of a sentence. For example, a dependency parse might identify that a particular verb is a noun, and specifically that it is the subject noun of a sentence.

While this isn't strictly speaking information extraction (although it is structured prediction), having access to a dependency parse can be very valuable in extracting information from documents.

First, let's look at how a dependency parse encodes grammatical information by using spaCy's dependency visualizer.
"""

doc = nlp(articles[313]['body'])
sent = list(doc.sents)[1]
displacy.render(sent, style="dep", jupyter=True)

"""You can think about dependency parses as a greatly enhanced form of part of speech tagging. While part of speech tagging assigns labels to individual words, like "Russian" being an ADJ[ective], "jets" being a NOUN, and "traveled" being a VERB (the labels below each word), dependency parsing goes a step further and tells you that the noun "jet" is specifically the subject noun ("nsubj") of the verb "traveled", and that "Russian" is not only an adjective, but specifically an adjective that is modifying the word "jets".

The implementation of dependency parsers is beyond the scope of this tutorial. Dependency parsing is a more complicated task than named entity recognition, given that a model needs to infer a tree structure that is subject to constraints (e.g. each word can only have a single "head" word immediately above it in the tree), and also needs to predict the correct label for each relationship. A useful list of different implementations is available at [Papers With Code](https://paperswithcode.com/task/dependency-parsing).

### Example information extraction with dependency parses

On its own, a dependency parse doesn't give you the ability to extract information from documents. That said, the information within a dependency parse can help you with a rule-based for extracting information.

One thing we might want to be able to extract from text is generally what kinds of behaviors or actions are occurring in a particular location. Let's write a function to identify verbs + direct objects that are grammatically linked to a location.
"""

print(doc)
tok = doc[21]  # "Aleppo"
print(tok)


def loc_to_verb(tok):
    verb_phrase = []
    # first, iterate through all the ancesters of the token
    for i in tok.ancestors:
        # when you get to a verb (using a POS tag)...
        if i.pos_ == "VERB":
            # ...add the verb to the verb phrase list
            verb_phrase.append(i)
            # then, also add the direct object(s) of the verb, as long as the original token
            # is in the same subtree as the direct object
            verb_phrase.extend([j for j in i.children if j.dep_ == "dobj" and tok in i.subtree])
            # we only want the first verb, so stop after we find one
            break
    # expand out the verb phrase to get modifiers ("amod") of the direct object
    for i in verb_phrase:
        for j in i.children:
            if j.dep_ == "amod":
                verb_phrase.append(j)

    # sort the tokens by their position in the original sentence
    new_list = sorted(verb_phrase, key=lambda x: x.i)
    # join them together with the correct whitespace and return
    return ''.join([i.text_with_ws for i in new_list]).strip()

loc_to_verb(tok)

"""We can then use our function to identify all the actions related to a single city, Aleppo."""

aleppo_actions = []

for d in docs:
    for i in d:
        if i.text == "Aleppo":
            aleppo_actions.append(loc_to_verb(i))

sorted(list(set(aleppo_actions)))

"""## Exercise: who's attacking whom?

We saw in our earlier Aleppo example that many of the events occurring in Aleppo involves military attacks, for example, "launching counter-attacks" or "resumed assault". A researcher might want to know who's attacking and who is being attacked, either to help understand the contours of the conflict or to generate quantitative data on the military course of the conflict.

Spend some time writing a function to identify who's attacking whom in the corpus.

Hints:

- you can use the `lemma_` attribute on a spaCy token to get its lemma form, so, for example, taking the lemma of "attack", "attacked", and "attacking" will all be "attack".
- think the role of subject nouns, direct objects, and prepositional phrases.
- the `spacy.explain()` function can help explain labels (e.g. `spacy.explain("amod") --> "adjectival modifier"`

"""

# look for attack events between entities

def find_attacks(doc):
    attacks = []
    # Iterate through each sentence in the document
    for sent in doc.sents:
        # Iterate through each token in the sentence
        for token in sent:
            # Check if the token is a verb and its lemma is "attack"
            if token.pos_ == 'VERB' and token.lemma_ == 'attack':
                subject = None
                target = None
                # Check the children of the verb token
                for child in token.children:
                    # Identify the subject of the verb
                    if child.dep_ in ['nsubj', 'nsubjpass']:
                        subject = child.text
                    # Identify the direct object or the object of a preposition (target)
                    elif child.dep_ in ['dobj', 'pobj']:
                        target = child.text
                if subject and target:
                    attacks.append((subject, target))
    return attacks

# Example usage
text = "Rebels are attacking the city. The army attacked the insurgents in response."
doc = nlp(text)
attacks_info = find_attacks(doc)
print(attacks_info)

"""## Summary: Extracting Information with Rules and Dependency Parses

What we've done is an example of rule-based information extraction using dependency parses as a component in our rule-based system. An alternative approach to rule-based extraction is to train a machine learning model to extract the information you're looking for. Machine learning models are often more accurate and less sensitive to small changes in language than rule-based systems are. Rule-based systems have their place, though. Researchers do not need to annotate large amounts of data to create rule-based systems, and it's easier to understand why a system returned the answer it did than is often the case with machine learning systems.

# Extentions and Experiments

### Transformer-based spaCy model

How does the accuracy change if you use the model with pretrained embeddings (`en_core_web_lg`) or the transformer-based model (`en_core_web_trf`)? (If you use the transformer model, you'll probably want to change your runtime to GPU, which will require you to re-install the libraries you installed at the beginning of the notebook).
"""

!python -m spacy download en_core_web_trf

nlp_trf = spacy.load("en_core_web_trf")

doc = nlp_trf(articles[313]['body'])
displacy.render(doc, style="ent", jupyter=True)

"""### Using Transformers from Hugging face 🤗 for NER"""

from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

nlp = pipeline("ner", model=model, tokenizer=tokenizer)
example = "My name is Wolfgang and I live in Berlin"

ner_results = nlp(example)
print(ner_results)

"""### Question-Answering

As we discussed briefly above, the "question-answering" framework can be very effective for extracting information, especially in a zero-shot situation where we have no additional training data.

One popular QA training dataset is SQuAD2, and we can download a transformer model that's already been trained on SQuAD from the Huggingface model repository.
"""

display(sent.text)

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline

model_name = "deepset/roberta-base-squad2"

hugg = pipeline('question-answering', model=model_name, tokenizer=model_name)

QA_input = {
    'question': "Who controls Deir Hafer and Al-Bab?",
    'context': sent.text
}
res = hugg(QA_input)

print(res)

"""**Questions**

- What kinds of tasks can you frame in terms of question answering?
- Why might you not want to use a QA approach?
- What happens if you give the model a different question? What about a nonsensical question?
"""